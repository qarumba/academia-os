ACADEMIAOS 2.0 - SERVICES LAYER ARCHITECTURE

1. SERVICE OVERVIEW

The services layer implements business logic for AI-powered qualitative research using the Service Layer Pattern. All services are static classes providing global accessibility with centralized configuration management through localStorage.

Core Services:
- ChatService: Multi-provider AI chat model management
- EmbeddingService: Vector embeddings and similarity search
- OpenAIService: Gioia methodology implementation and qualitative analysis
- ModelService: Model configuration validation and management
- HeliconeService: AI API monitoring and cost tracking
- LangChainService: LangChain framework integration
- PDFService: Document processing and text extraction
- SearchService: Academic paper search via Semantic Scholar
- RankingService: Paper relevance scoring and ordering

2. CHATSERVICE - MULTI-PROVIDER AI ABSTRACTION

Purpose: Unified interface for OpenAI and Anthropic models with graceful fallbacks

Key Methods:
- getModelConfig(): Retrieve configuration from localStorage
- createChatModel(options): Create configured chat model instance
- isAnthropicAvailable(): Check native Anthropic integration status
- handleError(): Centralized error handling with user feedback
- getEmbeddings(): Get OpenAI embeddings instance (compatibility)

LangChain v0.3 Integration:
- Dynamic Anthropic package loading with async import
- Native @langchain/anthropic support with initialization detection
- Updated import paths: @langchain/openai, @langchain/core/messages
- Improved constructor patterns for LangChain v0.3 API

Configuration Handling:
- Reads provider, model, and API keys from localStorage
- Automatic fallback from Anthropic to OpenAI when needed
- Helicone integration for both providers with provider-specific endpoints
- Transparent error messaging for unsupported configurations
- Advanced debugging with development mode logging

Architectural Pattern:
- Static factory methods for model creation
- Provider abstraction with runtime capability detection
- Configuration-driven behavior with validation
- Centralized error handling with user-friendly messages

3. EMBEDDINGSERVICE - VECTOR EMBEDDINGS MANAGEMENT

Purpose: Unified interface for creating vector embeddings using OpenAI

Key Methods:
- createEmbeddings(): Create configured embeddings instance
- embedQuery(text): Generate embedding for single text
- embedDocuments(texts): Generate embeddings for multiple texts

LangChain v0.3 Integration:
- Updated import: @langchain/openai for OpenAIEmbeddings
- Modern constructor pattern with merged configuration objects
- Enhanced error handling for missing API keys

Implementation Details:
- Always uses OpenAI embeddings for compatibility
- Supports both OpenAI and Anthropic primary models
- Requires OpenAI key for embeddings when using Anthropic
- Helicone integration for cost tracking
- Provider-agnostic design with clear error messaging

4. OPENAISERVICE - GIOIA METHODOLOGY IMPLEMENTATION

Purpose: Complete qualitative analysis using Gioia methodology with LangChain v0.3

Core Gioia Methods:
- initialCodingOfPaper(paper, remarks): First-order coding
- secondOrderCoding(codes): Theme aggregation  
- aggregateDimensions(secondOrderCodes): Theoretical dimensions
- modelConstruction(modelData, remarks): Theory construction
- modelVisualization(modelData): Mermaid diagram generation
- critiqueModel(modelData): AI model critique

Supporting Methods:
- streamCompletion(prompt, callback): Real-time streaming
- getDetailAboutPaper(paper, detail): Information extraction
- findTentativeResearchQuestions(papers): Research question generation
- brainstormApplicableTheories(dimensions): Theory identification
- conceptTuples(modelData): Concept relationship generation
- findRelevantParagraphsAndSummarize(modelData, tuples): Evidence analysis

LangChain v0.3 Migration Features:
- Updated imports: @langchain/core/documents, @langchain/textsplitters
- Modern CharacterTextSplitter from dedicated package
- Compatible with both OpenAI and Anthropic via ChatService
- Enhanced error handling with model-agnostic approach

Implementation Features:
- Document chunking for large texts (10,000 character chunks)
- Vector similarity search for evidence gathering
- JSON response formatting for structured data
- Comprehensive error handling with user feedback
- Multi-provider model support via unified ChatService

5. MODELSERVICE - CONFIGURATION VALIDATION

Purpose: Validate and manage AI model configurations

Key Methods:
- isModelConfigured(): Check if valid configuration exists
- getModelConfig(): Retrieve current configuration
- hasOpenAIKey(): Verify OpenAI key availability

Validation Logic:
- Checks required fields (provider, model, apiKey)
- Validates Anthropic configurations require OpenAI embeddings key
- Supports legacy OpenAI key fallback
- JSON parsing with error handling

6. HELICONESERVICE - API MONITORING & COST TRACKING

Purpose: Monitor AI API usage, track costs, and provide analytics

Key Methods:
- isHeliconeConfigured(): Check configuration status
- getHeliconeConfigForProvider(provider): Get provider-specific config
- testConnection(): Validate API connectivity via server proxy
- getSessionStats(startTime): Fetch session-based statistics
- getRecentRequests(limit): Retrieve recent API calls
- safeApiCall(apiCall, fallback): Graceful API call handling

Server Integration Methods:
- Uses dedicated Node.js server for CORS-free API access
- API_BASE_URL: http://localhost:3001/api/helicone (dev) or /api/helicone (prod)
- POST endpoints for requests and connection testing
- Enhanced error handling with specific CORS messaging

Implementation Details:
- Provider-specific endpoints (OpenAI: oai.helicone.ai, Anthropic: anthropic.helicone.ai)
- Production-ready server-side proxy for CORS handling
- Real-time statistics with session tracking and reset capabilities
- Comprehensive error handling for network and server issues
- Development mode fallbacks for offline usage

7. LANGCHAINSERVICE - FRAMEWORK INTEGRATION

Purpose: Manage LangChain framework integration using singleton pattern

Key Methods:
- getInstance(): Get singleton instance
- getModel(modelKey): Retrieve cached model or create new
- clearCache(): Clear model cache for configuration changes

Implementation Details:
- Singleton pattern for framework management
- Model caching for performance optimization
- Configuration-driven model creation
- Future extensibility for additional providers

8. SEARCHSERVICE - ACADEMIC PAPER SEARCH

Purpose: Search and retrieve academic papers from Semantic Scholar API

Key Methods:
- searchPapers(query, limit): Search papers by query
- getPaperById(paperId): Retrieve specific paper details
- getRecommendations(paperId, limit): Get related papers
- searchByAuthor(authorName, limit): Find papers by author

Implementation Details:
- Semantic Scholar API integration
- Mock data fallback for development
- Error handling with graceful degradation
- Pagination support for large result sets

9. RANKINGSERVICE - PAPER RELEVANCE SCORING

Purpose: Rank academic papers by relevance using vector similarity

Key Methods:
- rankPapers(queryString, papers): Rank papers by relevance

LangChain v0.3 Implementation:
- Updated imports: @langchain/core/documents, @langchain/textsplitters
- Modern CharacterTextSplitter with proper constructor
- MemoryVectorStore integration (remains in main langchain package)
- EmbeddingService integration for multi-provider support

Implementation Process:
- Document chunking (1,000 characters with 50 character overlap)
- Vector embedding creation for papers and query
- Similarity search using MemoryVectorStore
- Relevance scoring and ordering
- Duplicate removal based on paper ID

Performance Features:
- Graceful error handling returns original order on failure
- Efficient vector operations using LangChain v0.3 utilities
- Memory-based vector store for fast similarity search
- Compatible with both OpenAI and Anthropic models

10. PDFSERVICE - DOCUMENT PROCESSING

Purpose: Extract text content from PDF documents

Key Methods:
- extractTextFromPDF(file): Browser-based text extraction using PDF.js
- extractTextWithOCR(file): OCR processing using Adobe PDF Extract API
- validatePDFFile(file): File validation (type, size limits)
- getPDFMetadata(file): Extract document metadata

Implementation Details:
- PDF.js integration for client-side processing
- Adobe PDF Extract API for scanned documents
- File validation (50MB size limit, PDF type checking)
- Comprehensive error handling for corrupted files
- Base64 encoding for API submissions
- Compatible with modern LangChain text processing pipeline

11. ARCHITECTURAL PATTERNS

Service Layer Benefits:
- Separation of concerns between UI and business logic
- Centralized configuration management
- Multi-provider abstraction with graceful fallbacks
- Comprehensive monitoring and analytics
- Consistent error handling patterns
- Future extensibility for additional AI providers

Technology Integration:
- LangChain framework for AI model orchestration
- Vector embeddings for semantic search
- LocalStorage for configuration persistence
- External API integration (Semantic Scholar, Helicone, Adobe)
- Real-time streaming for AI responses

Error Handling Strategy:
- Centralized error logging and reporting
- Graceful degradation for network failures
- User-friendly error messages
- Fallback mechanisms for service unavailability
- Development mode mock data support

This architecture enables enterprise-grade AI-powered qualitative research capabilities while maintaining code quality, extensibility, and research methodology accuracy.

Last Updated: June 8, 2025
LangChain v0.3.27 Migration: Complete